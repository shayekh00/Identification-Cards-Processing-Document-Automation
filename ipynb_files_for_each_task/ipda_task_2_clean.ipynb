{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"curlEQIf0ihE"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vECc1JMc0vjQ"},"outputs":[],"source":["import os\n","import tensorflow as tf\n","import numpy as np\n","import cv2 as cv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IWhcKl33opu"},"outputs":[],"source":["# Defining meta-information for model\n","IMG_WIDTH = 256\n","IMG_HEIGHT = 256\n","interpolation_method = cv.INTER_AREA # Reasoning: seems to close gaps well but results in smoother lines - nice for these longer lines you have with your id\n","\n","# Defining Paths\n","dirname = os.path.dirname(__file__)\n","base_path = os.path.join(dirname, 'correct_path_to_dataset/2_segmentation/2_segmentation/') # define the correct path\n","train_path = base_path + 'Train/'\n","test_path = base_path + 'Test/'\n","\n","train_ids_dir = os.path.join(train_path, 'Ids')\n","train_masks_dir = os.path.join(train_path, 'GroundTruth')\n","\n","train_ids_list = sorted(os.listdir(train_ids_dir))\n","train_masks_list = sorted(os.listdir(train_masks_dir))\n","\n","test_ids_dir = os.path.join(test_path, 'Ids')\n","test_masks_dir = os.path.join(test_path, 'GroundTruth')\n","\n","test_ids_list = sorted(os.listdir(test_ids_dir))\n","test_masks_list = sorted(os.listdir(test_masks_dir))\n","\n","\n","# Handle Training Data\n","train_images = []\n","train_masks = []\n","\n","for img_name, mask_name in zip(train_ids_list, train_masks_list):\n","    img_path = os.path.join(train_ids_dir, img_name)\n","    mask_path = os.path.join(train_masks_dir, mask_name)\n","\n","    # Load and preprocess images\n","    img = cv.imread(img_path)\n","    img = cv.resize(img,(IMG_WIDTH,IMG_HEIGHT),interpolation=interpolation_method)\n","    train_images.append(img)\n","\n","    # Load and preprocess masks (assuming they are grayscale)\n","    mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n","    mask = cv.resize(mask,(IMG_WIDTH,IMG_HEIGHT),interpolation=interpolation_method)\n","    train_masks.append(mask)\n","\n","# Convert lists to numpy arrays\n","train_images = np.array(train_images)\n","train_masks = np.array(train_masks)\n","\n","# Normalize pixel values if needed (e.g., scale to [0, 1]) (done in next code cell)\n","train_images = train_images / 255.0\n","train_masks = train_masks / 255.0\n","\n","\n","# Handle Testing Data\n","test_images = []\n","test_masks = []\n","\n","for img_name, mask_name in zip(test_ids_list, test_masks_list):\n","    img_path = os.path.join(test_ids_dir, img_name)\n","    mask_path = os.path.join(test_masks_dir, mask_name)\n","\n","    # Load and preprocess images\n","    img = cv.imread(img_path)\n","    img = cv.resize(img,(IMG_WIDTH,IMG_HEIGHT),interpolation=interpolation_method)\n","    test_images.append(img)\n","\n","    # Load and preprocess masks (assuming they are grayscale)\n","    mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n","    mask = cv.resize(mask,(IMG_WIDTH,IMG_HEIGHT),interpolation=interpolation_method)\n","    test_masks.append(mask)\n","\n","# Convert lists to numpy arrays\n","test_images = np.array(test_images)\n","test_masks = np.array(test_masks)\n","\n","# Normalize pixel values if needed (e.g., scale to [0, 1]) (done in next code cell)\n","test_images = test_images / 255.0\n","test_masks = test_masks / 255.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NvE28KY0Efa-"},"outputs":[],"source":["# Defining our U-Net (4 down, 1 bottleneck, 4 up)\n","IMG_CHANNELS = 3\n","\n","inputs = tf.keras.layers.Input((IMG_WIDTH,IMG_HEIGHT,IMG_CHANNELS))\n","\n","# downsampling\n","c1 = tf.keras.layers.Conv2D(16, (3,3), activation='relu', padding='same')(inputs)\n","c1 = tf.keras.layers.Dropout(0.05)(c1)\n","c1 = tf.keras.layers.Conv2D(16, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n","p1 = tf.keras.layers.MaxPooling2D((2,2))(c1)\n","\n","c2 = tf.keras.layers.Conv2D(32, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n","c2 = tf.keras.layers.Dropout(0.05)(c2)\n","c2 = tf.keras.layers.Conv2D(32, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n","p2 = tf.keras.layers.MaxPooling2D((2,2))(c2)\n","\n","c3 = tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n","c3 = tf.keras.layers.Dropout(0.1)(c3)\n","c3 = tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n","p3 = tf.keras.layers.MaxPooling2D((2,2))(c3)\n","\n","c4 = tf.keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n","c4 = tf.keras.layers.Dropout(0.15)(c4)\n","c4 = tf.keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n","p4 = tf.keras.layers.MaxPooling2D((2,2))(c4)\n","\n","c5 = tf.keras.layers.Conv2D(256, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n","c5 = tf.keras.layers.Dropout(0.15)(c5)\n","c5 = tf.keras.layers.Conv2D(256, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n","\n","# upsampling\n","u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2))(c5)\n","u6 = tf.keras.layers.concatenate([u6, c4])\n","c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n","c6 = tf.keras.layers.Dropout(0.1)(c6)\n","c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n","\n","u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2))(c6)\n","u7 = tf.keras.layers.concatenate([u7, c3])\n","c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n","c7 = tf.keras.layers.Dropout(0.1)(c7)\n","c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n","\n","u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2))(c7)\n","u8 = tf.keras.layers.concatenate([u8, c2])\n","c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n","c8 = tf.keras.layers.Dropout(0.05)(c8)\n","c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n","\n","u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2))(c8)\n","u9 = tf.keras.layers.concatenate([u9, c1])\n","c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n","c9 = tf.keras.layers.Dropout(0.05)(c9)\n","c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n","\n","outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n","\n","model = None\n","model = tf.keras.Model(inputs=inputs, outputs=outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wa_fBAEyFBes"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bi3ucY4ZFVxS"},"outputs":[],"source":["# https://stackoverflow.com/questions/63952338/how-to-save-best-weights-and-best-model-using-keras\n","checkpoint = None\n","callbacks_list = None\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('/content/drive/My Drive/ipda/models/ipda_task_2_clean_best_weights.h5', monitor='val_binary_io_u_4', verbose=1, save_best_only=True,  mode='max')\n","callbacks_list = [checkpoint]\n","\n","# Why adam for optimizer? Generally good performance according to https://stackoverflow.com/questions/37214884/how-do-i-choose-an-optimizer-for-my-tensorflow-model and common use in practice according to lecture slides\n","# Why binary_focal_crossentropy for loss? We work with binary segmentation (imbalanced problem).\n","# Why accuracy for metrics? Imbalanced problem, so we add Recall to the mix insteadof just accuracy\n","# https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU\n","model.compile(optimizer='adam', loss='binary_focal_crossentropy', metrics=['accuracy','Recall',tf.keras.metrics.BinaryIoU(target_class_ids=[1], threshold=0.5)])\n","fitting_information = model.fit(train_images, train_masks, validation_split=0.2, epochs=100, batch_size=16, callbacks=callbacks_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3171,"status":"ok","timestamp":1707235722351,"user":{"displayName":"Richard L","userId":"01739195207295655374"},"user_tz":-60},"id":"N2gYj4w4Jjpv","outputId":"8cb1b5a1-d0cc-427b-e49e-84e2150e3da8"},"outputs":[{"name":"stdout","output_type":"stream","text":["4/4 [==============================] - 1s 68ms/step - loss: 0.0059 - accuracy: 0.9904 - recall: 0.9397 - binary_io_u_4: 0.9471\n","training loss: 0.0008193336543627083, training accuracy: 0.9937513470649719, training recall: 0.9707491397857666, training binary_io_u: 0.9680373072624207\n","validation loss: 0.010075886733829975, validation accuracy: 0.9886512756347656, validation recall: 0.9129956960678101, validation binary_io_u: 0.9271654486656189\n","test loss: 0.0058863540180027485, test accuracy: 0.9904333353042603, test recall: 0.9397119879722595, test binary_io_u: 0.947098970413208\n"]}],"source":["# Test model and print validation and test accuracies\n","model.load_weights('/content/drive/My Drive/ipda/models/ipda_task_2_clean_best_weights.h5')\n","test_loss, test_acc, test_recall, test_binary_io_u = model.evaluate(test_images, test_masks)\n","\n","print(f\"training loss: {fitting_information.history['loss'][-1]}, training accuracy: {fitting_information.history['accuracy'][-1]}, training recall: {fitting_information.history['recall'][-1]}, training binary_io_u: {fitting_information.history['binary_io_u_4'][-1]}\")\n","print(f\"validation loss: {fitting_information.history['val_loss'][-1]}, validation accuracy: {fitting_information.history['val_accuracy'][-1]}, validation recall: {fitting_information.history['val_recall'][-1]}, validation binary_io_u: {fitting_information.history['val_binary_io_u_4'][-1]}\")\n","print(f\"test loss: {test_loss}, test accuracy: {test_acc}, test recall: {test_recall}, test binary_io_u: {test_binary_io_u}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YW_tp0SVgkd7"},"outputs":[],"source":["# Save Model\n","model_save_path = os.path.join(dirname,'models/ipda_task_2_clean_6')\n","model.save(model_save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"envufKCYnhCg"},"outputs":[],"source":["# Performing Segmentation w/ Mask\n","threshold_value = 0.2\n","\n","def threshold_mask(mask,threshold_value):\n","    return np.where(mask >= threshold_value, 1, 0).astype(np.uint8)\n","\n","def resize_mask_to_image(image, mask):\n","    # image (shape: (256, 256, 3)) and mask (shape: ((256, 256, 1))) are both np.arrays\n","    image_height, image_width = image.shape[:2]\n","    resized_mask = cv.resize(mask, (image_width, image_height), interpolation=cv.INTER_CUBIC)\n","    return resized_mask\n","\n","def apply_mask_to_image(image, mask):\n","    # Resize Mask\n","    resized_mask = resize_mask_to_image(image, mask)\n","\n","    # apply thresholding\n","    thresholded_mask = threshold_mask(resized_mask,threshold_value)\n","\n","    masked_image = cv.bitwise_and(image, image, mask=thresholded_mask)\n","    return masked_image\n","\n","def create_bounding_box(mask):\n","    thresholded_mask = threshold_mask(mask,threshold_value)\n","\n","    # Find contours from the mask\n","    contours, _ = cv.findContours(thresholded_mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n","\n","    # Get the bounding box coordinates of the largest contour\n","    if contours:\n","        largest_contour = max(contours, key=cv.contourArea)\n","        x, y, w, h = cv.boundingRect(largest_contour)\n","        return x, y, w, h\n","    else:\n","        return None\n","\n","def crop_image_using_mask(image, mask):\n","    # Resize Mask\n","    resized_mask = resize_mask_to_image(image, mask)\n","\n","    # Create bounding box around the mask\n","    x, y, w, h = create_bounding_box(resized_mask)\n","\n","    if x is not None:\n","        # Crop the image using the bounding box coordinates\n","        cropped_image = image[y:y+h, x:x+w]\n","        return cropped_image\n","    else:\n","        return None"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOnCPm5R22R8DncJUQek5eL","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
