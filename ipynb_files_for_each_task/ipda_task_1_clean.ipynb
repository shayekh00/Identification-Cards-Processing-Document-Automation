{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mW4vPbEgnbyM"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"entoI0DOuUWZ"},"outputs":[],"source":["import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2YhxXOSniUo"},"outputs":[],"source":["# Defining meta-information for model\n","IMG_WIDTH = 128\n","IMG_HEIGHT = 128\n","interpolation_method = 'lanczos' # Reasoning: keeps finer details in downsampling | didn't really do much for how well it predicts\n","#interpolation_method = 'nearest' # Reasoning: keeps finer details in downsampling\n","\n","# Defining Paths\n","dirname = os.path.dirname(__file__)\n","base_path = os.path.join(dirname, 'correct_path_to_dataset/1_classification/1_classification/') # define the correct path\n","train_path = base_path + 'Train/'\n","test_path = base_path + 'Test/'\n","\n","train_csv = train_path + 'labels_train.csv'\n","test_csv = test_path + 'labels_test.csv'\n","\n","# Read CSV files\n","train_labels = pd.read_csv(train_csv)\n","test_labels = pd.read_csv(test_csv)\n","\n","# Define image data generators for train and test sets\n","#train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,validation_split=0.2,brightness_range=[1.0,1.5])\n","train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,validation_split=0.2)\n","test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n","\n","# Get images from directories using data generators\n","train_generator = train_datagen.flow_from_directory(\n","    train_path,\n","    target_size=(IMG_WIDTH, IMG_HEIGHT),  # Set your image dimensions\n","    batch_size=32,\n","    class_mode='binary',\n","    subset='training',\n","    #interpolation=interpolation_method,\n","    shuffle=True)  # For binary classification\n","\n","validation_generator = train_datagen.flow_from_directory(\n","    train_path,\n","    target_size=(IMG_WIDTH, IMG_HEIGHT),  # Set your image dimensions\n","    batch_size=32,\n","    class_mode='binary',\n","    subset='validation',\n","    #interpolation=interpolation_method,\n","    shuffle=True)  # For binary classification\n","\n","test_generator = test_datagen.flow_from_directory(\n","    test_path,\n","    target_size=(IMG_WIDTH, IMG_HEIGHT),  # Set your image dimensions\n","    batch_size=32,\n","    #interpolation=interpolation_method,\n","    class_mode='binary')  # For binary classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3SpRpSUBuD48"},"outputs":[],"source":["IMG_WIDTH = 128\n","IMG_HEIGHT = 128\n","# Defining Model\n","model = models.Sequential()\n","model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(0.01)))\n","model.add(layers.MaxPooling2D((3, 3)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.005)))\n","model.add(layers.MaxPooling2D((4, 4)))\n","model.add(layers.Flatten())\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dropout(0.3))\n","model.add(layers.Dense(1, activation='sigmoid'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hnUpVP39zkPk"},"outputs":[],"source":["# Show Model Summary\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHqYfRDx06PW"},"outputs":[],"source":["# Why adam for optimizer? Generally good performance according to https://stackoverflow.com/questions/37214884/how-do-i-choose-an-optimizer-for-my-tensorflow-model and common use in practice according to lecture slides\n","# Why binary_crossentropy for loss? We work with binary classification on a balanced dataset (labels 1 and 0 occur each 400 and 100 times in the train and test datasets respectively).\n","# Why accuracy for metrics? Again, balanced dataset so accuracy is appropriate\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.BinaryIoU(target_class_ids=[0], threshold=0.5)]) # 0 for some reason true labels\n","fitting_information = model.fit(train_generator, epochs=8,validation_data=validation_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGABVUZO4mCO"},"outputs":[],"source":["# Test model and print validation and test accuracies\n","test_loss, test_acc, test_binary_io_u = model.evaluate(test_generator, verbose=2)\n","\n","print(f\"training loss: {fitting_information.history['loss'][-1]}, training accuracy: {fitting_information.history['accuracy'][-1]}, training binary_io_u: {fitting_information.history['binary_io_u_3'][-1]}\")\n","print(f\"validation loss: {fitting_information.history['val_loss'][-1]}, validation accuracy: {fitting_information.history['val_accuracy'][-1]}, validation binary_io_u: {fitting_information.history['val_binary_io_u_3'][-1]}\")\n","print(f\"test loss: {test_loss}, test accuracy: {test_acc}, test binary_io_u: {test_binary_io_u}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LtbDC3--Uyb"},"outputs":[],"source":["# Save Model\n","model_save_path = os.path.join(dirname,'models/ipda_task_1_clean_18')\n","model.save(model_save_path)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMPPFsZy9K3cX06YmWbm//3","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
